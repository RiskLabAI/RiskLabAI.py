import pandas as pd
import numpy as np
from enum import Enum
from tqdm.notebook import tqdm
from numpy.random import RandomState
from collections import Counter
import itertools
import sklearn.metrics as Metrics
from sklearn.preprocessing import minmax_scale

# -------------------------------
# Custom ClassificationExperiment
# -------------------------------
# This class mimics pycaretâ€™s interface but is implemented entirely with sklearn.
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.dummy import DummyClassifier
try:
    from xgboost import XGBClassifier
except ImportError:
    XGBClassifier = DummyClassifier
try:
    from lightgbm import LGBMClassifier
except ImportError:
    LGBMClassifier = DummyClassifier

class ClassificationExperiment:
    def setup(self, data, target, fold, train_size, session_id, n_jobs, normalize, normalize_method):
        # Store the data and parameters; also perform normalization if requested.
        self.data = data.copy()
        self.target = target.copy()
        self.fold = fold
        self.train_size = train_size
        self.session_id = session_id
        self.n_jobs = n_jobs
        self.normalize = normalize
        self.normalize_method = normalize_method
        if normalize:
            if normalize_method == 'zscore':
                self.data = (data - data.mean()) / data.std()
            elif normalize_method == 'minmax':
                self.data = (data - data.min()) / (data.max() - data.min())
        return self

    def models(self):
        # Returns a DataFrame listing available model references.
        available_models = {
            'lr': 'lr',
            'knn': 'knn',
            'nb': 'nb',
            'dt': 'dt',
            'svm': 'svm',
            'rbfsvm': 'rbfsvm',
            'gpc': 'gpc',
            'mlp': 'mlp',
            'ridge': 'ridge',
            'rf': 'rf',
            'qda': 'qda',
            'ada': 'ada',
            'gbc': 'gbc',
            'lda': 'lda',
            'et': 'et',
            'xgboost': 'xgboost',
            'lightgbm': 'lightgbm',
            'dummy': 'dummy'
        }
        df = pd.DataFrame({'Reference': list(available_models.keys())}, 
                          index=list(available_models.keys()))
        return df

    def compare_models(self, n_select, include):
        # For simplicity, this method selects models from a predefined dict based on the include list.
        available_models = {
            'lr': LogisticRegression(),
            'knn': KNeighborsClassifier(),
            'nb': GaussianNB(),
            'dt': DecisionTreeClassifier(),
            'svm': SVC(probability=True),
            'rbfsvm': SVC(probability=True, kernel='rbf'),
            'gpc': GaussianProcessClassifier(),
            'mlp': MLPClassifier(),
            'ridge': RidgeClassifier(),
            'rf': RandomForestClassifier(),
            'qda': QuadraticDiscriminantAnalysis(),
            'ada': AdaBoostClassifier(),
            'gbc': GradientBoostingClassifier(),
            'lda': LinearDiscriminantAnalysis(),
            'et': ExtraTreesClassifier(),
            'xgboost': XGBClassifier(),
            'lightgbm': LGBMClassifier(),
            'dummy': DummyClassifier()
        }
        selected_models = []
        for item in include:
            if isinstance(item, str):
                if item in available_models:
                    selected_models.append(available_models[item])
            else:
                # If already a model instance, include it.
                selected_models.append(item)
        # Select the first n_select models if needed.
        if isinstance(n_select, int) and n_select < len(selected_models):
            selected_models = selected_models[:n_select]
        return selected_models

    def tune_model(self, model, return_tuner=False, fold=4, verbose=True, tuner_verbose=True, 
                   optimize='Accuracy', search_library='scikit-optimize', search_algorithm='bayesian', 
                   n_iter=25, choose_better=True, custom_grid=None):
        # A minimal tuning implementation.
        from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
        if custom_grid is not None and len(custom_grid) > 0:
            tuner = GridSearchCV(model, custom_grid, cv=fold)
            tuner.fit(self.data, self.target)
            tuned_model = tuner.best_estimator_
        else:
            tuner = None
            tuned_model = model
        if return_tuner:
            return tuned_model, tuner
        else:
            return tuned_model

    def finalize_model(self, estimator, model_only=True):
        # Dummy finalize: simply returns the estimator.
        return estimator

# -------------------------------
# Feature Importance Functions
# -------------------------------
# (For models that require the feature names, we add a check to set classifier.feature_names_in_ if needed.)
def feature_importance_linear_models(
    classifier: 'sklearn.base.BaseEstimator',
    X: pd.DataFrame, 
    y,
) -> pd.Series:
    classifier.fit(X, y)
    if isinstance(X, pd.DataFrame) and not hasattr(classifier, 'feature_names_in_'):
        classifier.feature_names_in_ = X.columns
    coefficients_importances = np.abs(classifier.coef_).mean(axis=0)
    coefficients_importances = pd.Series(coefficients_importances, classifier.feature_names_in_)
    importances_scaled = minmax_scale(
        coefficients_importances,
        feature_range=(0, 1),
        axis=0,
    )
    return pd.Series(importances_scaled, index=coefficients_importances.index)

def feature_importance_sklearn(
    classifier: 'sklearn.base.BaseEstimator',
    X: pd.DataFrame, 
    y,
) -> pd.Series:
    classifier.fit(X, y)
    if isinstance(X, pd.DataFrame) and not hasattr(classifier, 'feature_names_in_'):
        classifier.feature_names_in_ = X.columns
    importances = pd.Series(classifier.feature_importances_, index=classifier.feature_names_in_)
    importances = pd.concat({
        "Mean": importances.mean(),
    }, axis=1)
    importances_scaled = minmax_scale(
        importances["Mean"],
        feature_range=(0, 1),
        axis=0,
    )
    return pd.Series(importances_scaled, index=importances.index)

def feature_importance_RFE(
    classifier: 'sklearn.base.BaseEstimator',
    X: pd.DataFrame, 
    y,
) -> pd.Series:
    from sklearn.feature_selection import RFE
    rfe = RFE(
        estimator=classifier,
        verbose=0,
        n_features_to_select=1,    
    )
    rfe.fit(X, y)
    if isinstance(X, pd.DataFrame) and not hasattr(rfe, 'feature_names_in_'):
        rfe.feature_names_in_ = X.columns
    inverted_ranking = np.max(rfe.ranking_) - rfe.ranking_ + 1
    normalized_importance = minmax_scale(inverted_ranking)
    return pd.Series(normalized_importance, index=rfe.feature_names_in_)

def feature_importance_SFI(
    classifier: 'sklearn.base.BaseEstimator',
    X: pd.DataFrame, 
    y,
    n_splits: int = 5,
    score_sample_weights: np.ndarray = None,  
    train_sample_weights: np.ndarray = None, 
) -> pd.Series:
    from sklearn.model_selection import StratifiedKFold 
    if train_sample_weights is None:
        train_sample_weights = np.ones(X.shape[0])
    if score_sample_weights is None:
        score_sample_weights = np.ones(X.shape[0])
    cv_generator = StratifiedKFold(n_splits=n_splits)
    feature_names = X.columns
    importances = []
    for feature_name in feature_names:
        scores = []
        for train, test in cv_generator.split(X, y):
            x_train, y_train, weights_train = X.iloc[train, :][[feature_name]], y.iloc[train], train_sample_weights[train]
            x_test, y_test, weights_test = X.iloc[test, :][[feature_name]], y.iloc[test], score_sample_weights[test]
            try:
                classifier.fit(x_train, y_train, sample_weight=weights_train)
            except:
                classifier.fit(x_train, y_train)
            prediction = classifier.predict(x_test)
            score = Metrics.accuracy_score(y_test, prediction, sample_weight=weights_test)
            scores.append(score)
        importances.append({
            "FeatureName": feature_name,
            "Mean": np.mean(scores),
        })
    importances = pd.DataFrame(importances)
    importances_scaled = minmax_scale(
        importances["Mean"],
        feature_range=(0, 1),
        axis=0,
    )
    return pd.Series(importances_scaled, index=importances.FeatureName)

def feature_importance_MDI(
    classifier: 'sklearn.base.BaseEstimator',
    X: pd.DataFrame, 
    y,
) -> pd.Series:
    classifier.fit(X, y)
    if isinstance(X, pd.DataFrame) and not hasattr(classifier, 'feature_names_in_'):
        classifier.feature_names_in_ = X.columns
    importances = pd.Series(classifier.feature_importances_, index=classifier.feature_names_in_)
    importances_scaled = minmax_scale(
        importances,
        feature_range=(0, 1),
        axis=0,
    )
    return pd.Series(importances_scaled, index=importances.index)

def feature_importance_MDA(
    classifier: 'sklearn.base.BaseEstimator',
    X: pd.DataFrame, 
    y,
    n_repeats: int = 5,
) -> pd.Series:
    from sklearn.inspection import permutation_importance
    classifier.fit(X, y)
    importances = permutation_importance(classifier, X, y, n_repeats=n_repeats, random_state=43).importances_mean
    importances = minmax_scale(
        importances,
        feature_range=(0, 1),
        axis=0,
    )
    return pd.Series(importances, index=X.columns)

# -------------------------------
# Univariate Analysis Functions
# -------------------------------
class UAMeasure(Enum):
    SPEARMAN = "Spearman"
    PEARSON = "Pearson"
    KENDAL_TAU = "Kendal-Tau"
    MUTUAL_INFORMATION = "Mutual Information"
    ANOVA_F = "ANOVA F-Stat"

def feature_importance_univariate_analysis(measurements, X, y):
    from scipy.stats import pearsonr, kendalltau
    from sklearn.feature_selection import mutual_info_classif, f_classif
    ua = pd.DataFrame(data=None, index=X.columns)
    if UAMeasure.SPEARMAN.value in measurements:
        spearmans = minmax_scale(X.apply(lambda feature: np.abs(np.corrcoef(feature, y)[0, 1]), axis=0))
        ua[UAMeasure.SPEARMAN.value] = spearmans
    if UAMeasure.PEARSON.value in measurements:
        pearsons = minmax_scale(X.apply(lambda feature: np.abs(pearsonr(feature, y).statistic), axis=0))
        ua[UAMeasure.PEARSON.value] = pearsons
    if UAMeasure.KENDAL_TAU.value in measurements:
        kendalltaus = minmax_scale(X.apply(lambda feature: np.abs(kendalltau(feature, y).statistic), axis=0))
        ua[UAMeasure.KENDAL_TAU.value] = kendalltaus
    if UAMeasure.MUTUAL_INFORMATION.value in measurements:
        mutual_informations = minmax_scale(mutual_info_classif(X, y, n_neighbors=51))
        ua[UAMeasure.MUTUAL_INFORMATION.value] = mutual_informations
    if UAMeasure.ANOVA_F.value in measurements:
        f_values = minmax_scale(f_classif(X, y)[0])
        ua[UAMeasure.ANOVA_F.value] = f_values
    return ua

def full_name(klass):
    return ".".join([klass.__module__, klass.__name__])

# -------------------------------
# Enums for FI Model Types
# -------------------------------
class FIModel(Enum):
    COEFFICIENT_BASED = "Coefficient-Based + Shrinkage & Selection"
    TREE_BASED = "Tree-Based"
    PERMUTATION_BASED = "Permutation-Based"
    SINGLE_FEATURE_BASED = "Single Feature-Based"
    RFE_BASED = "Recursive Feature Elimination-Based"

# -------------------------------
# SWEFI Class
# -------------------------------
class SWEFI:
    def __init__(self, X: pd.DataFrame, y, n_fold:int =10):
        self.percentage = None
        # Use our custom ClassificationExperiment instead of pycaret
        self.clfx = ClassificationExperiment().setup(data=X, target=y, fold=n_fold, train_size=0.99, 
                                                       session_id=123, n_jobs=-1, normalize=True, normalize_method='zscore')
        self.X = X
        self.y = y

    def select_models(self, select_n_model: 'Union[int, float]' = 100):
        model_to_methods = self.clfx.models()
        model_to_methods['FI Methods'] = pd.Series({
            'lr': [FIModel.COEFFICIENT_BASED.value, FIModel.PERMUTATION_BASED.value, FIModel.SINGLE_FEATURE_BASED.value, FIModel.RFE_BASED.value],
            'knn': [FIModel.PERMUTATION_BASED.value],
            'nb': [FIModel.SINGLE_FEATURE_BASED.value],
            'dt': [FIModel.SINGLE_FEATURE_BASED.value],
            'svm': [FIModel.COEFFICIENT_BASED.value, FIModel.PERMUTATION_BASED.value],
            'rbfsvm': [FIModel.PERMUTATION_BASED.value],
            'gpc': [FIModel.PERMUTATION_BASED.value],
            'mlp': [FIModel.PERMUTATION_BASED.value],
            'ridge': [FIModel.COEFFICIENT_BASED.value, FIModel.PERMUTATION_BASED.value, FIModel.SINGLE_FEATURE_BASED.value, FIModel.RFE_BASED.value],
            'rf': [FIModel.TREE_BASED.value, FIModel.RFE_BASED.value],
            'qda': [FIModel.PERMUTATION_BASED.value],
            'ada': [FIModel.PERMUTATION_BASED.value],
            'gbc': [FIModel.PERMUTATION_BASED.value],
            'lda': [FIModel.COEFFICIENT_BASED.value, FIModel.PERMUTATION_BASED.value, FIModel.SINGLE_FEATURE_BASED.value, FIModel.RFE_BASED.value],
            'et': [FIModel.TREE_BASED.value, FIModel.RFE_BASED.value],
            'xgboost': [FIModel.TREE_BASED.value, FIModel.RFE_BASED.value],
            'lightgbm': [FIModel.PERMUTATION_BASED.value],
            'dummy': [FIModel.PERMUTATION_BASED.value],
        })
        model_to_methods = model_to_methods[['Reference', 'FI Methods']].set_index('Reference').squeeze().to_dict()
        model_to_methods.update({
            "sklearn.svm._classes.SVC": [FIModel.COEFFICIENT_BASED.value, FIModel.RFE_BASED.value],
        })
        self.model_to_methods = model_to_methods
        exclude = set(['dummy', 'svm', 'knn', 'nb', 'lightgbm', 'ada', 'gpc', 'rbfsvm'])
        all_models = set(self.clfx.models().index.tolist())
        external_models = [SVC(probability=True, kernel='linear')]
        include = list(all_models - exclude) + external_models
        if type(select_n_model) == float:
            self.learning_models = self.clfx.compare_models(n_select=round(len(self.clfx.models()) * select_n_model),
                                                            include=include)
        else:
            self.learning_models = self.clfx.compare_models(n_select=select_n_model,
                                                            include=include)
        return self

    def select_univariate_analysis_measurements(self, measurements: list = ['Pearson', 'Spearman', 'Kendal']):
        self.measurements = measurements
        return self

    def fine_tune_selected_models(self, hpo_n_fold:int=4, hpo_n_iter:int=25, hpo_metric:str='Accuracy', 
                                  hpo_search_library:str='scikit-optimize', hpo_search_algorithm='bayesian'):
        learning_models = self.learning_models
        model_to_methods = self.model_to_methods
        model_to_custom_config = {
            "sklearn.ensemble._forest.ExtraTreesClassifier": None,
            "sklearn.neighbors._classification.KNeighborsClassifier": None,
            "sklearn.ensemble._forest.RandomForestClassifier": None,
            "xgboost.sklearn.XGBClassifier": None,
            "lightgbm.sklearn.LGBMClassifier": {},
            "sklearn.neural_network._multilayer_perceptron.MLPClassifier": None,
            "sklearn.ensemble._gb.GradientBoostingClassifier": {},
            "sklearn.naive_bayes.GaussianNB": {},
            "sklearn.ensemble._weight_boosting.AdaBoostClassifier": {},
            "sklearn.linear_model._ridge.RidgeClassifier": {
                "alpha": [1],
                "solver": ["lsqr"],
            },
            "sklearn.discriminant_analysis.LinearDiscriminantAnalysis": {
                "shrinkage": [0.0],
                "solver": ["lsqr"],
            },
            "sklearn.linear_model._logistic.LogisticRegression": {
                "C": [1],
                "solver": ['liblinear'],
            },
            "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis": {},
            "sklearn.tree._classes.DecisionTreeClassifier": None,
            "sklearn.svm._classes.SVC": {
                'probability': [True],
                'C': [1.0],
                'kernel': ["linear"],
            },
        }
        tuned_learning_models = []
        for model in learning_models:
            print("-" * 80)
            print(model.__class__.__name__)
            if model_to_custom_config[full_name(model.__class__)] is None:
                tlm, tuner = self.clfx.tune_model(
                    model,
                    return_tuner=True,
                    fold=hpo_n_fold, 
                    verbose=True, 
                    tuner_verbose=True, 
                    optimize=hpo_metric, 
                    search_library=hpo_search_library, 
                    search_algorithm=hpo_search_algorithm,
                    n_iter=hpo_n_iter,
                    choose_better=True
                )
            elif len(model_to_custom_config[full_name(model.__class__)]) == 0:
                print("No hyper-tuning ...")
                tlm = model
            else:
                print("Custom config ...")
                tlm, tuner = self.clfx.tune_model(
                    model,
                    return_tuner=True,
                    fold=hpo_n_fold, 
                    verbose=True, 
                    tuner_verbose=True, 
                    optimize=hpo_metric, 
                    search_library='scikit-learn', 
                    search_algorithm='grid',
                    custom_grid=model_to_custom_config[full_name(model.__class__)], 
                    n_iter=hpo_n_iter,
                    choose_better=True
                )
            tuned_learning_models.append(tlm)
        self.finalized_tuned_learning_models = [self.clfx.finalize_model(estimator=tlm, model_only=True) for tlm in tuned_learning_models]
        model_method_pairs = []
        for tlm in tuned_learning_models:
            methods = model_to_methods[full_name(tlm.__class__)]
            for method in methods:
                model_method_pairs.append((tlm, method))
        self.model_method_pairs = model_method_pairs
        return self

    def _initialize_model_feature_importance_data(self, models: list, n_iteration: int = 5):
        feature_names = self.X.columns
        index_columns = ["Model Name", "Feature Importance Model", "Subset Index"]
        to_apply_product_list = [
            models,
            list(range(n_iteration)),
        ]
        tuples = [(type(model).__name__, proper_fi_model, iteration) 
                  for (model, proper_fi_model), iteration in itertools.product(*to_apply_product_list)]
        index = pd.MultiIndex.from_tuples(tuples, names=index_columns)
        data = pd.DataFrame({}, columns=feature_names, index=index)
        return data, index

    def _initialize_univariate_analysis_feature_importance_data(self, measurements: list, n_iteration: int = 5):
        feature_names = self.X.columns
        index_columns = ["Model Name", "Feature Importance Model", "Subset Index"]
        to_apply_product_list = [
            measurements,
            list(range(n_iteration)),
        ]
        tuples = [("Univariate Analysis", ua_measurement, iteration) 
                  for (ua_measurement,), iteration in itertools.product(*to_apply_product_list)]
        index = pd.MultiIndex.from_tuples(tuples, names=index_columns)
        data = pd.DataFrame({}, columns=feature_names, index=index)
        return data, index

    @staticmethod
    def stationary_bootstrap(X: pd.DataFrame, y, n_iteration: int = 5):
        from arch.bootstrap import optimal_block_length, StationaryBootstrap
        optimal_block_size = round(optimal_block_length(X).mean(axis=0)["stationary"])
        random_state = RandomState(1234)
        bootstraper = StationaryBootstrap(optimal_block_size, X=X, y=y, random_state=random_state)
        for bootstraped_data in bootstraper.bootstrap(n_iteration):
            yield bootstraped_data[1]['X'], bootstraped_data[1]['y']

    @staticmethod
    def iid_bootstrap(X: pd.DataFrame, y, n_iteration: int = 5):
        from arch.bootstrap import IIDBootstrap
        random_state = RandomState(1234)
        bootstraper = IIDBootstrap(X=X, y=y, random_state=random_state)
        for bootstraped_data in bootstraper.bootstrap(n_iteration):
            yield bootstraped_data[1]['X'], bootstraped_data[1]['y']

    def compute_feature_importance_data(self, bootstrap_method, n_iteration: int = 10, n_repeats: int = 10):
        X = self.X
        y = self.y
        X = (X - X.mean()) / X.std()
        model_method_pairs = self.model_method_pairs
        models_fi, models_index = self._initialize_model_feature_importance_data(model_method_pairs, n_iteration=n_iteration)
        measurements = self.measurements
        measurements_fi, measurements_index = self._initialize_univariate_analysis_feature_importance_data(measurements, n_iteration=n_iteration)
        for iteration, (X_train, y_train) in tqdm(enumerate(bootstrap_method(X, y, n_iteration)), total=n_iteration):
            for model, proper_fi_model in model_method_pairs:
                if proper_fi_model == FIModel.TREE_BASED.value:
                    try:
                        result = feature_importance_MDI(model, X_train, y_train).values
                    except:
                        result = feature_importance_sklearn(model, X_train, y_train).values
                elif proper_fi_model == FIModel.PERMUTATION_BASED.value:
                    result = feature_importance_MDA(model, X_train, y_train, n_repeats=n_repeats).values
                elif proper_fi_model == FIModel.COEFFICIENT_BASED.value:
                    result = feature_importance_linear_models(model, X_train, y_train).values
                elif proper_fi_model == FIModel.SINGLE_FEATURE_BASED.value:
                    result = feature_importance_SFI(model, X_train, y_train, n_splits=n_repeats).values
                elif proper_fi_model == FIModel.RFE_BASED.value:
                    result = feature_importance_RFE(model, X_train, y_train).values
                result = (result - result.min()) / (result.max() - result.min()) 
                models_fi.loc[type(model).__name__, proper_fi_model, iteration] = result
            ua_result = feature_importance_univariate_analysis(measurements, X_train, y_train).T.values
            measurements_fi.loc[pd.IndexSlice["Univariate Analysis", :, iteration]] = ua_result
        self.models_fi = models_fi
        self.models_index = models_index
        self.measurements_fi = measurements_fi
        self.measurements_index = measurements_index
        self.n_iterationn = n_iteration
        return self

    def compute_swefi_scores(self, percentage: float = 0.5, weight: str = 'linear'):
        features = self.X.columns
        fi = pd.concat([self.models_fi, self.measurements_fi])
        index = self.models_index.tolist() + self.measurements_index.tolist()
        k = round(len(features) * percentage)
        important_features_together = []
        for idx in index:
            important_features_in_current_step = fi.loc[idx].squeeze().sort_values(ascending=False)[:k].index.tolist()
            important_features_together.extend(important_features_in_current_step)
        feature_ranked_as_k_top_important = dict(zip(features, [0] * len(features)))
        n_times_that_feature_ranked_as_important = Counter(important_features_together)
        dict.update(feature_ranked_as_k_top_important, n_times_that_feature_ranked_as_important)
        feature_ranked_as_k_top_important = pd.Series(feature_ranked_as_k_top_important)
        stability_scores = feature_ranked_as_k_top_important / (self.n_iterationn * (len(self.model_method_pairs) + len(self.measurements)))
        if weight == 'linear':
            weights = stability_scores / stability_scores.sum()
            swefi = (fi * weights)
        elif weight == 'logarithmic':
            weights = np.log(stability_scores + 1)
            weights = weights / weights.sum()
            swefi = (fi * weights)
        elif weight == 'exponential':
            weights = np.exp(stability_scores)
            weights = weights / weights.sum()
            swefi = (fi * weights)
        elif weight == 'harmonic':
            swefi = (2 * fi * stability_scores) / (fi + stability_scores)
        elif weight == 'power-2':
            weights = stability_scores ** 2
            weights = weights / weights.sum()
            swefi = (fi * weights)
        elif weight == 'power-0.5':
            weights = (stability_scores ** 0.5)
            weights = weights / weights.sum()
            swefi = (fi * weights)
        elif weight == 'entropy':
            weights = (-stability_scores * np.log(stability_scores))
            weights = weights / weights.sum()            
            swefi = (fi * weights)
        else:
            raise NotImplementedError(f"Weight {weight} not implemented.")
        swefi = pd.concat(
            [swefi.mean() / swefi.mean().sum(), swefi.std() * (swefi.shape[0]) ** -0.5 / swefi.mean().sum()],
            axis=1,
        ).rename(columns={0: 'mean(SWEFI)', 1: 'std(SWEFI)'}).sort_values(by='mean(SWEFI)')
        self.stability_scores = stability_scores
        self.swefi = swefi
        return self

    def get_swefi_scores(self):
        return self.swefi

    def get_inner_models_feature_importances(self):
        models_fi = self.models_fi.groupby(level=[0, 1]).sum()
        models_fi = models_fi.div(models_fi.sum(axis=1).values, axis=0).transpose()
        return models_fi
